A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.[1][2] The largest and most capable LLMs are generative pre-trained transformers (GPTs) and provide the core capabilities of chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering.[3] These models acquire predictive power regarding syntax, semantics, and ontologies[4] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.[5]

They consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.[6]

LLMs evolved from earlier statistical and recurrent neural network approaches to language modeling. The transformer architecture, introduced in 2017, replaced recurrence with self-attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes.[7] This innovation enabled models like GPT, BERT, and their successors, which demonstrated emergent behaviors at scale such as few-shot learning and compositional reasoning.[8]

Reinforcement learning, particularly policy gradient algorithms, has been adapted to fine-tune LLMs for desired behaviors beyond raw next-token prediction.[9] Reinforcement learning from human feedback (RLHF) applies these methods to optimize a policy, the LLM's output distribution, against reward signals derived from human or automated preference judgments.[10] This has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance.

Benchmark evaluations for LLMs have evolved from narrow linguistic assessments toward comprehensive, multi-task evaluations measuring reasoning, factual accuracy, alignment, and safety.[11][12] Hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements.[13]

Applications in specific domains
Large language models have achieved impressive results across multiple specialized domains beyond dialogue, demonstrating capacity for knowledge transfer and domain-specific adaptation.

In software development, LLMs power intelligent development tools facilitating code completion, automated programming, and software engineering assistance without requiring special tokenization for programming languages. Models trained on mixed natural language and source code corpora demonstrate bidirectional proficiency, generating code from natural language specifications, translating between programming languages, and explaining code logic.[14]

In computational biology, transformer-based architectures have revolutionized protein structure prediction and analysis, with embedding-based methods running an order of magnitude faster than MSA-based approaches while maintaining comparable accuracy.[15] Meta Platforms' ESMFold methodology produces protein structure predictions at unprecedented scale, underpinning the ESM Atlas database containing 772 million metagenomic protein structures.[16] LLM architectures have demonstrated capacity to design novel proteins with no natural analogues, suggesting potential applications in synthetic biology and biotechnology.[17] In nucleic acid analysis, models effectively identify regulatory sequences, perform sequence classification, predict RNA-RNA interactions, and characterize RNA secondary structures.[18]

Clinical and mental health contexts present emerging applications alongside significant safety concerns, with survey data indicating that nearly half of individuals with existing mental health conditions have used LLMs for emotional support or therapeutic purposes.[19] However, this application domain manifests multiple limitations, including hallucination-driven misinformation in sensitive contexts, inadequate safety protocols for suicide risk assessment, and documented instances of models expressing stigmatizing perspectives on mental health disorders.[20]

History

The number of publications about large language models by year grouped by publication types.

The training compute of notable large models in FLOPs vs publication date over the period 2010–2024. For overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within leading companies (bottom right). The majority of these models are language models.

The training compute of notable large AI models in FLOPs vs publication date over the period 2017–2024. The majority of large models are language models or multimodal models with language capacity.
Before the emergence of transformer-based models in 2017, some language models were considered large relative to the computational and data constraints of their time. In the early 1990s, IBM's statistical models pioneered word alignment techniques for machine translation, laying the groundwork for corpus-based language modeling. In 2001, a smoothed n-gram model, such as those employing Kneser–Ney smoothing, trained on 300 million words, achieved state-of-the-art perplexity on benchmark tests.[21] During the 2000s, with the rise of widespread internet access, researchers began compiling massive text datasets from the web ("web as corpus"[22]) to train statistical language models.[23][24]

Moving beyond n-gram models, researchers started in 2000 to use neural networks to learn language models.[25] Following the breakthrough of deep neural networks in image classification around 2012,[26] similar architectures were adapted for language tasks. This shift was marked by the development of word embeddings (eg, Word2Vec by Mikolov in 2013) and sequence-to-sequence (seq2seq) models using LSTM. In 2016, Google transitioned its translation service to neural machine translation (NMT), replacing statistical phrase-based models with deep recurrent neural networks. These early NMT systems used LSTM-based encoder-decoder architectures, as they preceded the invention of transformers.


An illustration of main components of the transformer model from the original paper, where layers were normalized after (instead of before) multiheaded attention
At the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper "Attention Is All You Need". This paper's goal was to improve upon 2014 seq2seq technology,[27] and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014.[28] The following year in 2018, BERT was introduced and quickly became "ubiquitous".[29] Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.[30]

Although decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use.[31] GPT-3 in 2020 went a step further and as of 2025 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing chatbot ChatGPT that received extensive media coverage and public attention.[32] The 2023 GPT-4 was praised for its increased accuracy and as a "holy grail" for its multimodal capabilities.[33] OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work.[30] In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer.[34] Many LLMs with parameter counts comparable to those of OpenAI's GPT series have been developed.[35]

Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower cost.[36]

Since 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).[37]

As of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).[38][39][40]

Open-source LLMs have increasingly shaped the field since 2023, contributing to broader participation in AI development and greater transparency in model evaluation. Vake et al. (2025) demonstrated that community-driven contributions to open-source models measurably improve their efficiency and performance, with user participation growing rapidly on collaborative platforms such as Hugging Face.[41] Paris et al. (2025) further argued that openness in AI should extend beyond releasing model code or weights to encompass inclusiveness, accountability, and ethical responsibility in AI research and deployment.[42] Collectively, these studies highlight that open-source LLMs can accelerate innovation and enhance scientific reproducibility, while fostering a more transparent and participatory AI ecosystem.